---
title: "ISA 401: Business Intelligence & Data Visualization"
subtitle: "11: Data Correction and Imputation"
author: '<br>Fadel M. Megahed, PhD <br><br>Professor of Information Systems and Business Analytics <br> Farmer School of Business<br> Miami University<br><br> [`r icons::icon_style(icons::fontawesome("twitter"), fill = "white")` @FadelMegahed](https://twitter.com/FadelMegahed) <br> [`r icons::icon_style(icons::fontawesome("github"), fill = "white")` fmegahed](https://github.com/fmegahed/) <br> [`r icons::icon_style(icons::fontawesome("paper-plane", style = "solid"), fill = "white")` fmegahed@miamioh.edu](mailto:fmegahed@miamioh.edu)<br> [`r icons::icon_style(icons::fontawesome("question"), fill = "white")` Automated Scheduler for Office Hours](https://calendly.com/fmegahed)<br><br>'
date: "Fall 2024"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [default, "../../style_files/fonts.css", "../../style_files/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
header-includes:  
  - "../../style_files/header.html"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE,
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dev = 'png',
                      fig.height = 2.5,
                      dpi = 300,
                      fig.align = 'center')

options(htmltools.dir.version = FALSE)

miamired = '#C3142D'

if(require(pacman)==FALSE) install.packages("pacman")
if(require(devtools)==FALSE) install.packages("devtools")

if(require(countdown)==FALSE) devtools::install_github("gadenbuie/countdown")
if(require(xaringanExtra)==FALSE) devtools::install_github("gadenbuie/xaringanExtra")


pacman::p_load(tidyverse, magrittr, lubridate, janitor, # data analysis pkgs
               httr, jsonlite, tidycensus, # for APIs
               fontawesome, RefManageR, xaringanExtra, countdown) # for slides
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
if(require(xaringanthemer) == FALSE) install.packages("xaringanthemer")
library(xaringanthemer)

style_mono_accent(base_color = "#84d6d3",
                  base_font_size = "20px")

xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons", "panelset", "search", "fit_screen", "editable", "clipable"))
```


# Quick Refresher from Last Class


`r emo::ji("check")` Converting tidy data to technically correct data  

`r emo::ji("check")` Examining the consistency of data 


---

# Learning Objectives for Today's Class

- Correct errors (with transformation rules, deductive correction and deterministic imputation)  

- Describe different scenarios for missing data  

- Apply some basic imputation techniques for missing data (e.g., mean/median/mode, kNN, etc)


---
class: center, inverse, middle

# Error Correction <br>(i.e., Correcting Inconsistencies in the Data)


---

# Correction

> Correction methods aim to fix inconsistent observations by **altering invalid values in a record based on information from valid values**. Depending on the method this is either a **single-step procedure** or a **two-step procedure** where first, an error localization method is used to empty certain fields, followed by an imputation step. --- [De Jonge and Van Der Loo (2013)](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf)


---

# Simple Transformation Rules

In practice, data cleaning procedures involve a lot of **ad-hoc transformations**. This may lead to long scripts where one selects parts of the data, changes some variables, selects another part, changes some more variables, etc. 

- When such scripts are neatly written and commented, they can almost be **treated as a log of the actions** performed by the analyst. 

- However, as scripts get longer it is **better to store the transformation rules separately and log which rule is executed on what record**.

One approach to **do** this in R is through the [deducorrect package](https://cran.r-project.org/web/packages/deducorrect/deducorrect.pdf).

```{r functions_determinstic_correction, echo=FALSE}
library(gt)
tidyr <- tribble(
  ~ type, ~ `package_name`, ~ `fun()`, ~ `description`, 
  "**rules**", "deducorrect", "correctionRules()", "Rules for determinstic correction",
  "**correction**", "deducorrect", "correctWithRules()", "Apply simple replacement rules to a data.frame."
  
)

tidyr |> 
  gt() |> 
  fmt_markdown(everything()) |> 
  cols_label(
    `type` = md("`type`"),
    `package_name` = md("`package`"),
    `fun()` = md("`function()`"),
    `description` = md("`description`"),
  ) |> 
  as_raw_html()

```


---

# Simple Transformation Rules: Example

.pull-left[
```{r rules_det_correction}
people = tibble::tibble(
  name = c('Tom Cruise', 'Serena Williams', 'Taylor Swift', 'Ariana Grande', 'The Rock'),
  height = c(172, 1.75, 69.3, 154, 6.25),
  unit = c('cm', 'm', 'inch', 'cm', 'ft')
)

u <- deducorrect::correctionRules(
  expression(
    ## 1-------
    if(unit == "cm") height <- height/100,
    ## 2-------
    if(unit == "inch") height <- height/39.37,
    ## 3-------
    if(unit == "ft") height <- height/3.28,
    ## 4-------
    unit <- "m"
  ))


```
Note that the rules **can also be stored in a txt file**.

]


.pull-right[
```{r rules_det_correction2}
people_corrected = deducorrect::correctWithRules( #<<
  rules = u, dat = people) #<<

people_corrected$corrected

people_corrected$corrections
```
]


---

# Deductive Correction

When the data you are analyzing is generated by people rather than machines or measurement devices, certain typical human-generated errors are likely to occur. Examples include: (a) **typing errors** in numbers, (b) **rounding errors** in numbers, and (c) **sign errors**.

These errors can be accounted for through the [deducorrect package](https://cran.r-project.org/web/packages/deducorrect/deducorrect.pdf).

```{r functions_dedu_correction, echo=FALSE}
library(gt)
tidyr <- tribble(
  ~ type, ~ `package_name`, ~ `fun()`, ~ `description`, 
  "**rule def**", "editrules", "editmatrix()", "A set of comparison operators representing a linear system of (in)equations. The function editmatrix generates an editmatrix from a character vector, or an expression vector.",
  "**typos**", "deducorrect", "correctTypos()", "Tries to detect and repair records that violate linear equality constraints by correcting simple typos",
  "**rounding**", "deducorrect", "correctRounding()", "Tries to detect and repair records that violate linear (in)equality constraints by correcting possible rounding errors",
  "**sign**", "deducorrect", "correctSigns()", "Correct sign errors and value interchanges in data records."
  
)

tidyr |> 
  gt() |> 
  fmt_markdown(everything()) |> 
  cols_label(
    `type` = md("`type`"),
    `package_name` = md("`package`"),
    `fun()` = md("`function()`"),
    `description` = md("`description`"),
  ) |> 
  as_raw_html()

```

---

# Deductive Correction: Correct Typos

```{r typos}
e <- editrules::editmatrix("x + y == z") # defining the rules
d <- data.frame(x = 123, y = 132, z = 246) # creating the data frame
cor <- deducorrect::correctTypos(e, d) # attempting to correct Typos #<< 
cor$corrected # new values for the data.frame
cor$corrections # the log of what has changed
```



---

# Deductive Correction: Correct Rounding Errors

```{r rounding}
e <- editrules::editmatrix("x + y == z") # defining the rules
d <- data.frame(x = 101, y = 100, z = 200) # creating the data frame
cor <- deducorrect::correctRounding(e, d) # attempting to correct Rounding #<< 
cor$corrected # new values for the data.frame (may not be unique)
cor$corrections # the log of what has changed
```



---

# Deductive Correction: Correct Sign Errors

```{r signs}
e <- editrules::editmatrix("x + y == z") # defining the rules
d <- data.frame(x = -100, y = 100, z = 200) # creating the data frame
cor <- deducorrect::correctSigns(e, d) # attempting to correct the sign #<< 
cor$corrected # new values for the data.frame
cor$corrections # the log of what has changed
```

---

# Non-Graded Activity

`r countdown(minutes = 3, seconds = 0, top = 0, font_size = "2em")`

.panelset[

.panel[.panel-name[Activity]


> _Over the next 3 minutes, please examine the following:_ 

- Which of the aforementioned `correct...()` functions can be used to correct the following situation?  
  1. `correctWithRules()` 
  2. `correctTypos()` 
  3. `correctSigns()`  
  4. `correctRounding()`

```{r correction_example}
d <- data.frame(x = 173, y = 200, z = 379) # creating the data frame
```

]

.panel[.panel-name[Your Solution]

```{r sol, include=FALSE}
complex_rule = deducorrect::correctionRules(
  expression(
    if(x + y != z){z = x + y}
  )
)

c2 = deducorrect::correctWithRules(rules = complex_rule, dat = d)
c2
```


.can-edit.key-activity1[

Please insert your solution below.

.............................. 

Why did you pick your answer?

.............................. 
]
]
]






---
class: center, inverse, middle

# Missing Data Handling <br>(Complete Cases, Col. Deletion, & Imputation)

---

# Types of Missing Data: MCAR

Missing data are typically grouped into three categories:  

- **Missing completely at random (MCAR):** When data are MCAR, the fact that the data are missing is independent of the observed and unobserved data. For example,  
  * If we were to perform annual health checks for all FSB students, some of you may have missing lab values because a batch of lab samples was processed improperly.  
  * Here, the missing data **reduce the analyzable population of the study and consequently, the statistical power, but do not introduce bias:** when data are MCAR, the **remaining data can be considered a simple random sample of the full data set** of interest. 
  * Hence, the `NA` data can be simply be handled by removing such observations. In R, the `na.omit()` can be used without the need for introducing any packages. We refer to this `NA` handling approach as **complete case analysis**.
  

.footnote[
<html>
<hr>
</html>

**Source:** Mack C, Su Z, Westreich D. Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, Third Edition [Internet]. Rockville (MD): Agency for Healthcare Research and Quality (US); 2018 Feb. Types of Missing Data. Available from: <https://www.ncbi.nlm.nih.gov/books/NBK493614/>
]


---

# Types of Missing Data: MAR

- **Missing at random (MAR):** When data are MAR, the fact that the **data are missing is systematically related to the observed but not the unobserved data**. For example,   
  * A dataset examining depression may encounter data that are MAR if male participants are less likely to complete a survey about depression severity than female participants.   
  * That is, if the probability of survey completion is related to their `sex` (which is fully observed) but **not the severity of their depression**, then the data may be regarded as MAR. 
  * Approaches to Handle `NAs` for MAR Data:    
      + Complete case analyses of MAR data may or may not result in bias.  
      + Impute (i.e., guess) the missing values based on a proper accounting of the known factors (e.g., their sex)


.footnote[
<html>
<hr>
</html>

**Source:** Mack C, Su Z, Westreich D. Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, Third Edition [Internet]. Rockville (MD): Agency for Healthcare Research and Quality (US); 2018 Feb. Types of Missing Data. Available from: <https://www.ncbi.nlm.nih.gov/books/NBK493614/>
]

---

# Types of Missing Data: MNAR

- **Missing not at random (MNAR):** When data are MNAR, the fact that the data are missing is **systematically related to the unobserved data**, that is, the missingness is related to events or factors which are not measured by the researcher. To extend the previous example,   
  * the depression registry may encounter data that are MNAR if participants with severe depression are more likely to refuse to complete the survey about depression severity.  
  * Approaches to Handle `NAs` for MAR Data:    
      + Complete case analyses of MAR data may or may not result in bias.  
      + Imputation approaches **may not be suitable since the missigness pattern is due to the impact of unobserved auxiliary variables**. 

.footnote[
<html>
<hr>
</html>

**Source:** Mack C, Su Z, Westreich D. Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, Third Edition [Internet]. Rockville (MD): Agency for Healthcare Research and Quality (US); 2018 Feb. Types of Missing Data. Available from: <https://www.ncbi.nlm.nih.gov/books/NBK493614/>
]


---

# The Bike Sharing Dataset

```{r bike_share_1}
bike_tbl = readr::read_csv('../../data/bike_sharing_data.csv') |> 
  # making their column names tidy
  janitor::clean_names() |> 
  # creating date, hours, and day of the week variables
  dplyr::mutate(datetime = lubridate::mdy_hm(datetime), date = lubridate::as_date(datetime), 
         hours = lubridate::hour(datetime), day_of_week =  lubridate::wday(date)) |>
  # moving them to the front
  dplyr::relocate(date, day_of_week, hours) |> 
  select(-c(datetime)) |> 
  # converting int columns to factors
  dplyr::mutate(dplyr::across(.cols = c(hours, season, holiday, workingday, weather),
                       .fn = as.character)) |> 
  dplyr::mutate(dplyr::across(.cols = c(hours, season, holiday, workingday, weather),
                       .fn = as.factor))
```


---

# Bike Sharing Data: Viz Missing [1]

```{r bike_share_2}
visdat::vis_dat(bike_tbl) # function from visdat #<<
```


---

# Bike Sharing Data: Viz Missing [2]

```{r bike_share_3}
visdat::vis_miss(bike_tbl) # function from visdat #<<
```


---

# Bike Sharing Data: Missing Relationships [1]

```{r bike_share_4, out.height=600, results='asis'}
bike_tbl_grouped = bike_tbl |> 
  dplyr::group_by(day_of_week) |> # group_by from dplyr #>>
  dplyr::summarise(nmissing = is.na(sources) |> sum()) # summarise from dplyr #>> 

bike_tbl_grouped |> DT::datatable(
  fillContainer = FALSE, options = list(pageLength = 4), height = 600)
```

---

# Bike Sharing Data: Missing Relationships [2]

```{r bike_share_5}
bike_tbl_grouped = bike_tbl |> 
  dplyr::group_by(day_of_week, hours) |> # group_by from dplyr #>>
  dplyr::summarise(nmissing = is.na(sources) |> sum()) # summarise from dplyr #>> 

bike_tbl_grouped |> DT::datatable(
  fillContainer = FALSE, options = list(pageLength = 4), height = 600)
```


---

# Some Useful Imputation Functions

```{r functions_imputation, echo=FALSE}
library(gt)
tidyr <- tribble(
  ~ type, ~ `package_name`, ~ `fun()`, ~ `description`, 
  "**identify NA cells**", 'base', 'is.na()', 'returns logicals T/F for each NA cell',
  "**identify NA in each col**", 'base & purrr', 'purrr::map_df(.x = df, .f= is.na) |> colSums()', 'returns total NA per column',
  "**rule-based deductions**", "deducorrect", "
deduImpute()", "Based on observed values and edit rules, impute as many variables deductively as possible.",
  "**replace NAs**", "tidyr", "replace_na(list(x = 0, y = 'unknown'))", "Replace/Impute NA with pre-configured values <br> (e.g., mean, median, VIM::maxCat(), etc.)",
"**across column imputations**", "VIM", "kNN()", "k-nearest neighbours for imputation",
"**across column imputations**", "VIM", "rangerImpute()", "use of random forests for imputation",
"**multiple imputation**", "mice", "mice()", " multivariate imputation by chained equations"
  
)

tidyr |> 
  gt() |> 
  fmt_markdown(everything()) |> 
  cols_label(
    `type` = md("`type`"),
    `package_name` = md("`package`"),
    `fun()` = md("`function()`"),
    `description` = md("`description`"),
  ) |> 
  as_raw_html()

```

---

# A Simple Demo Using a Modified Iris Dataset

Please refer to our in class demo for more details.

```{r iris_imputation, include=FALSE}
iris_df = iris
dplyr::glimpse(iris_df)

# plots the missing pcts by column
# from the output, the percent of missing data in each col is 0%
DataExplorer::plot_missing(data = iris_df)

# we can also compute that using the map_df() approach from the slides
fct_missing = 
  # returns a dataframe where each column contains FALSE and TRUE cells
  # FALSE == no missing data
  purrr::map_df(.x = iris_df, .f = is.na) |> 
  # up to here, we get the number of rows missing
  # below, we sum all the TRUE (coded in the backend as 1) to get the 
  # the total number of rows that are missing in each column
  colSums() |> 
  # dividing by the number of rows in the original data so that we can get a
  magrittr::divide_by(150)

pct_missing = fct_missing*100

# inducing missing
iris_missing = iris_df
iris_missing[1:10, 1] = NA # set first 10 rows of col 1 to NA as a demo

head(iris_missing, 20) # print the first 20 rows

# imputing the missing data with mean or median (terrible)
mean_col1 = mean(iris_missing$Sepal.Length, na.rm = T)
med_col1 = median(iris_missing$Sepal.Length, na.rm = T)

iris_bad_imputation = tidyr::replace_na(
  data = iris_missing, # data frame to be imputed
  replace = list(Sepal.Length = mean_col1) # a list of colnames = value for replacement
  )
head(iris_bad_imputation, 15)

# imputing the missing data with kNN
iris_knn = VIM::kNN(data = iris_missing, k = 5)

```


---

class: center, inverse, middle

# Recap

---

# Summary of Main Points

By now, you should be able to do the following:  

- Correct errors (with transformation rules, deductive correction and deterministic imputation)  

- Describe different scenarios for missing data  

- Apply some basic imputation techniques for missing data (e.g., mean/median/mode, kNN, etc)

---


# Supplementary Reading


.center[[<img src="../../figures/intro_data_cleaning_book.png" height="450px">](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf)]
* [From technically correct to consistent data](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf)

